{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75eeceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_d, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert model_d % num_heads == 0\n",
    "        self.head_d = model_d // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(model_d, model_d, bias=False)\n",
    "        self.wk = nn.Linear(model_d, model_d, bias=False)\n",
    "        self.wv = nn.Linear(model_d, model_d, bias=False)\n",
    "        \n",
    "        self.wo = nn.Linear(model_d, model_d, bias=False)\n",
    "        \n",
    "        self.scale = math.sqrt(self.head_d)\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        \"\"\"\n",
    "        x: (B, S, model_d)\n",
    "        attn_mask: (B, S)\n",
    "        \"\"\"\n",
    "        B, S, _ = x.shape\n",
    "        h = self.num_heads\n",
    "        q = self.wq(x).reshape(B, S, h, self.head_d).transpose(1, 2)\n",
    "        k = self.wk(x).reshape(B, S, h, self.head_d).transpose(1, 2)\n",
    "        v = self.wv(x).reshape(B, S, h, self.head_d).transpose(1, 2)\n",
    "        # now they have (B, h, S, head_d)\n",
    "        logits = torch.matmul(q, k.transpose(-1, -2)) / self.scale\n",
    "        masked_logits = torch.masked_fill(logits, attn_mask.reshape(B, 1, 1, S), -float(\"inf\"))\n",
    "        attention = F.softmax(masked_logits, dim=-1)  # (B, h, S, S)\n",
    "        attended = torch.matmul(attention, v)  # (B, h, S, head_d)\n",
    "        outputs = attended.transpose(1, 2).reshape(B, S, -1)\n",
    "        return self.wo(outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3899d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output max|diff| = 8.941e-08\n",
      "Max prob on masked keys (manual) = 0.000e+00\n",
      "✓ Functional parity and masking look good.\n",
      "\n",
      "Output max|diff| = 5.960e-08\n",
      "✓ Functional parity and masking look good.\n",
      "\n",
      "Output max|diff| = 6.706e-08\n",
      "Max prob on masked keys (manual) = 0.000e+00\n",
      "✓ Functional parity and masking look good.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Version-robust comparison against torch.nn.functional.multi_head_attention_forward\n",
    "import math\n",
    "import inspect\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def manual_attn_probs_with_module_weights(x_bsd, key_padding_mask, mha):\n",
    "    B, S, D = x_bsd.shape\n",
    "    h, d = mha.num_heads, mha.head_d\n",
    "    scale = math.sqrt(d)\n",
    "\n",
    "    Wq, Wk, Wv = mha.wq.weight, mha.wk.weight, mha.wv.weight  # (D, D)\n",
    "\n",
    "    q = (x_bsd @ Wq.t()).reshape(B, S, h, d).transpose(1, 2)  # (B,h,S,d)\n",
    "    k = (x_bsd @ Wk.t()).reshape(B, S, h, d).transpose(1, 2)\n",
    "    v = (x_bsd @ Wv.t()).reshape(B, S, h, d).transpose(1, 2)\n",
    "\n",
    "    logits = (q @ k.transpose(-1, -2)) / scale  # (B,h,S,S)\n",
    "    if key_padding_mask is not None:\n",
    "        logits = logits.masked_fill(key_padding_mask[:, None, None, :], float(\"-inf\"))\n",
    "    return logits.softmax(dim=-1)  # (B,h,S,S)\n",
    "\n",
    "def functional_call_compat(x_bsd, key_padding_mask, mha):\n",
    "    \"\"\"\n",
    "    Always call the functional with (S, B, D). Provide in_proj_weight for all versions.\n",
    "    If separate weights are supported, set use_separate_proj_weight=True and in_proj_weight=None.\n",
    "    Otherwise, concatenate Wq;Wk;Wv into in_proj_weight.\n",
    "    \"\"\"\n",
    "    B, S, D = x_bsd.shape\n",
    "    H = mha.num_heads\n",
    "    x_sbd = x_bsd.transpose(0, 1)  # (S,B,D)\n",
    "\n",
    "    sig = inspect.signature(F.multi_head_attention_forward)\n",
    "    params = set(sig.parameters.keys())\n",
    "\n",
    "    # Start with the minimal, widely supported args (in the right order via kwargs).\n",
    "    kwargs = {\n",
    "        \"query\": x_sbd,\n",
    "        \"key\": x_sbd,\n",
    "        \"value\": x_sbd,\n",
    "        \"embed_dim_to_check\": D,\n",
    "        \"num_heads\": H,\n",
    "        \"in_proj_weight\": None,      # ensure present for old signatures\n",
    "        \"in_proj_bias\": None,\n",
    "        \"bias_k\": None,\n",
    "        \"bias_v\": None,\n",
    "        \"add_zero_attn\": False,\n",
    "        \"dropout_p\": 0.0,\n",
    "        \"out_proj_weight\": mha.wo.weight,\n",
    "        \"out_proj_bias\": None,\n",
    "        \"training\": False,\n",
    "        \"need_weights\": False,\n",
    "    }\n",
    "    if \"key_padding_mask\" in params:\n",
    "        kwargs[\"key_padding_mask\"] = key_padding_mask\n",
    "    if \"attn_mask\" in params:\n",
    "        kwargs[\"attn_mask\"] = None\n",
    "    if \"is_causal\" in params:\n",
    "        kwargs[\"is_causal\"] = False\n",
    "\n",
    "    if \"use_separate_proj_weight\" in params:\n",
    "        # Use separate q/k/v weights; keep in_proj_weight=None\n",
    "        kwargs.update({\n",
    "            \"use_separate_proj_weight\": True,\n",
    "            \"q_proj_weight\": mha.wq.weight,\n",
    "            \"k_proj_weight\": mha.wk.weight,\n",
    "            \"v_proj_weight\": mha.wv.weight,\n",
    "            \"static_k\": None,\n",
    "            \"static_v\": None,\n",
    "        })\n",
    "        # Remove keys not in this version\n",
    "        kwargs = {k: v for k, v in kwargs.items() if k in params}\n",
    "        y_sbd, *_ = F.multi_head_attention_forward(**kwargs)\n",
    "    else:\n",
    "        # Older versions: concatenate into in_proj_weight (3D, D)\n",
    "        in_proj_weight = torch.cat([mha.wq.weight, mha.wk.weight, mha.wv.weight], dim=0)\n",
    "        kwargs[\"in_proj_weight\"] = in_proj_weight\n",
    "        # Remove unsupported keys\n",
    "        for k in [\"use_separate_proj_weight\", \"q_proj_weight\", \"k_proj_weight\", \"v_proj_weight\",\n",
    "                  \"static_k\", \"static_v\"]:\n",
    "            kwargs.pop(k, None)\n",
    "        kwargs = {k: v for k, v in kwargs.items() if k in params}\n",
    "        y_sbd, *_ = F.multi_head_attention_forward(**kwargs)\n",
    "\n",
    "    return y_sbd.transpose(0, 1)  # back to (B,S,D)\n",
    "\n",
    "def compare_with_functional(B=2, S=5, D=32, H=4, use_mask=True, tol=(1e-6, 1e-5)):\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    x = torch.randn(B, S, D)\n",
    "    key_padding_mask = torch.zeros(B, S, dtype=torch.bool)\n",
    "    if use_mask and S >= 3:\n",
    "        key_padding_mask[:, -1] = True\n",
    "    if use_mask and S >= 5 and B >= 2:\n",
    "        key_padding_mask[0, 1] = True\n",
    "\n",
    "    mha = MultiheadAttention(D, H)\n",
    "\n",
    "    # Your module output\n",
    "    y_mod = mha(x, key_padding_mask)\n",
    "\n",
    "    # Functional output\n",
    "    y_fun = functional_call_compat(x, key_padding_mask, mha)\n",
    "\n",
    "    # Compare outputs\n",
    "    max_diff = (y_mod - y_fun).abs().max().item()\n",
    "    print(f\"Output max|diff| = {max_diff:.3e}\")\n",
    "    assert torch.allclose(y_mod, y_fun, atol=tol[0], rtol=tol[1]), \"Outputs differ!\"\n",
    "\n",
    "    # Check masking via manual probabilities (version-agnostic)\n",
    "    probs = manual_attn_probs_with_module_weights(x, key_padding_mask, mha)  # (B,H,S,S)\n",
    "    if key_padding_mask.any():\n",
    "        avg_probs = probs.mean(dim=1).mean(dim=1)  # (B,S)\n",
    "        masked_vals = avg_probs[key_padding_mask]\n",
    "        if masked_vals.numel():\n",
    "            max_masked = masked_vals.max().item()\n",
    "            print(f\"Max prob on masked keys (manual) = {max_masked:.3e}\")\n",
    "            assert max_masked < 1e-6, \"Masked keys still receive attention!\"\n",
    "\n",
    "    print(\"✓ Functional parity and masking look good.\\n\")\n",
    "\n",
    "# Try a few configs\n",
    "compare_with_functional(B=2, S=5, D=32, H=4, use_mask=True)\n",
    "compare_with_functional(B=1, S=7, D=16, H=1, use_mask=False)\n",
    "compare_with_functional(B=3, S=4, D=24, H=3, use_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38507b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
